{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN ENSEMBLE MODELS - DATASET_VERSION_1\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess import get_data_generators\n",
    "from ai_models.ensemble_model import run_ensemble_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Prepare data generators\n",
    "train_generator, validation_generator, test_generator, label_encoder, class_weights = get_data_generators()\n",
    "\n",
    "# Run the ensemble model\n",
    "run_ensemble_model(train_generator, validation_generator, test_generator, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN EFFICIENTNETB3 MODEL - DATASET_VERSION_1\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess import get_data_generators\n",
    "from ai_models.efficientnetB3_model import train_efficientnetb3, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_gen, val_gen, test_gen, label_enc, class_wts = get_data_generators()\n",
    "\n",
    "# Train EfficientNetB3\n",
    "model, history = train_efficientnetb3(train_gen, val_gen, class_wts)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN EFFICIENTNETB3 MODEL - DATASET_VERSION_2\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess_2 import get_data_generators\n",
    "from ai_models.efficientnetB3_model import train_efficientnetb3, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Train EfficientNetB3\n",
    "model, history = train_efficientnetb3(train_generator, validation_generator, class_weights)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN EFFICIENTNETB3 MODEL - DATASET_VERSION_3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess_3 import get_data_generators\n",
    "from ai_models.efficientnetB3_model import train_efficientnetb3, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Train EfficientNetB3\n",
    "model, history = train_efficientnetb3(train_generator, validation_generator, class_weights)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN DENSENET121 MODEL - DATASET-VERSION-3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess_3 import get_data_generators\n",
    "from ai_models.densenet121_model import train_densenet121_model, evaluate_densenet121_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Train densenet model\n",
    "model, history = train_densenet121_model(train_generator, validation_generator, class_weights)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_densenet121_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN RESNET50 MODEL - DATASET-VERSION-3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess_3 import get_data_generators\n",
    "from ai_models.resnet50_model import train_resnet50, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Train resnet model\n",
    "model, history = train_resnet50(train_generator, validation_generator, class_weights)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN RESNET101 MODEL - DATASET-VERSION-3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess import get_data_generators\n",
    "from ai_models.resnet101_model import train_resnet101, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Train resnet101 model\n",
    "model, history = train_resnet101(train_generator, validation_generator, class_weights)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SINGLE CUSTOM-CNN MODEL\n",
    "import sys\n",
    "import pandas as pd\n",
    "from utils.preprocess import get_data_generators\n",
    "from ai_models.cnn_model import train_custom_cnn, evaluate_model\n",
    "sys.path.append(r'D:\\skin_disease_detection\\backend\\app')\n",
    "\n",
    "# Load data generators\n",
    "train_generator, validation_generator, test_generator, label_encoder, class_weights = get_data_generators()\n",
    "\n",
    "# Train the model\n",
    "model, history = train_custom_cnn(train_generator, validation_generator)\n",
    "\n",
    "# Evaluate on validation set\n",
    "evaluate_model(model, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING ENSEMBLE MULTI-MODEL PREDICTIONS - worst\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "sys.path.append(r'D:\\skin_disease_detection\\trained_models')\n",
    "\n",
    "# Load base models\n",
    "resnet_model = load_model(r\"D:\\skin_disease_detection\\trained_models\\ResNet50_model.h5\")\n",
    "print(\"ResNet50 model loaded successfully!\")\n",
    "efficientnet_model = load_model(r\"D:\\skin_disease_detection\\trained_models\\EfficientNetB0_model.h5\")\n",
    "print(\"EfficientNet model loaded successfully!\") \n",
    "densenet_model = load_model(r\"D:\\skin_disease_detection\\trained_models\\DenseNet121_model.h5\")\n",
    "print(\"DenseNet model loaded successfully!\")\n",
    "\n",
    "# Load meta-model\n",
    "meta_model = load(r\"D:\\skin_disease_detection\\trained_models\\meta_model.pkl\")\n",
    "print(\"Meta-model loaded successfully!\")\n",
    "\n",
    "# Load an image\n",
    "image_path = r\"D:\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_images_part_2\\ISIC_0029360.jpg\"\n",
    "img = load_img(image_path, target_size=(224, 224))  # Resize as per model input size\n",
    "print(img)\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Convert single image to batch\n",
    "img_array = img_array / 255.0 \n",
    "\n",
    "# Get predictions from the base models\n",
    "resnet_pred = resnet_model.predict(img_array)\n",
    "efficientnet_pred = efficientnet_model.predict(img_array)\n",
    "densenet_pred = densenet_model.predict(img_array)\n",
    "\n",
    "print(\"ResNet50 Prediction:\", resnet_pred)\n",
    "print(\"EfficientNet Prediction:\", efficientnet_pred)\n",
    "print(\"DenseNet Prediction:\", densenet_pred)\n",
    "\n",
    "# Combine predictions for the meta-model\n",
    "base_predictions = np.array([resnet_pred, efficientnet_pred, densenet_pred]).reshape(1, -1)\n",
    "\n",
    "# Get the final prediction from the meta-model\n",
    "meta_prediction = meta_model.predict(base_predictions)\n",
    "print(\"Meta-Model Final Prediction:\", meta_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING SINGLE-MODEL EFICIENTNETB3 PREDICTIONS - WORST\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "MODEL_PATH = r'D:\\skin_disease_detection\\trained_models\\efficientnetb3.h5'  # Update as needed\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Define class labels (ensure these match your training labels)\n",
    "class_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']  # Confirm correct order\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess an image for model prediction.\"\"\"\n",
    "    img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array / 255.0  # Normalize\n",
    "    return img_array\n",
    "\n",
    "def predict_skin_disease(image_path):\n",
    "    \"\"\"Predict skin disease and print detailed output.\"\"\"\n",
    "    img_array = preprocess_image(image_path)\n",
    "    \n",
    "    # Get raw predictions\n",
    "    predictions = model.predict(img_array)\n",
    "    \n",
    "    # Print softmax probabilities for debugging\n",
    "    print(\"\\n########## SOFTMAX PROBABILITIES ##########\")\n",
    "    for i, prob in enumerate(predictions[0]):\n",
    "        print(f\"{class_labels[i]}: {prob:.4f}\")\n",
    "\n",
    "    # Get most likely class\n",
    "    predicted_class = np.argmax(predictions)  \n",
    "    confidence = np.max(predictions)  \n",
    "\n",
    "    # Map to class label\n",
    "    predicted_label = class_labels[predicted_class]\n",
    "\n",
    "    print(f\"\\nPredicted Class: {predicted_label} (Confidence: {confidence:.2f})\")\n",
    "\n",
    "# Test with a real image\n",
    "TEST_IMAGE_PATH = r'D:\\skin_disease_detection\\backend\\own_data\\base_dir\\test_dir\\df\\ISIC_0024330.jpg' # Update as needed\n",
    "predict_skin_disease(TEST_IMAGE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTNG MODEL PREDICTIONS ON A SINGLE IMAGE - working properly\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# Optionally, if you have a mapping from indices to class names:\n",
    "# Replace the dictionary below with your actual class mapping.\n",
    "class_indices = {\n",
    "    \"akiec\": 0,\n",
    "    \"bcc\": 1,\n",
    "    \"bkl\": 2,\n",
    "    \"df\": 3,\n",
    "    \"mel\": 4,\n",
    "    \"nv\": 5,\n",
    "    \"vasc\": 6\n",
    "}\n",
    "# Reverse mapping from index to class name\n",
    "idx2class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Loads an image from disk, converts it from BGR to RGB,\n",
    "    resizes it to target_size, preprocesses it for EfficientNet,\n",
    "    and expands dimensions to create a batch of size 1.\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found or cannot be read: \" + image_path)\n",
    "    # Convert BGR to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Resize image\n",
    "    image = cv2.resize(image, target_size)\n",
    "    # Preprocess using EfficientNet preprocessing function\n",
    "    image = preprocess_input(image)\n",
    "    # Expand dimensions to match the model's input (batch_size, height, width, channels)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "# Load your saved model\n",
    "model_path = r\"D:\\skin_disease_detection\\trained_models\\resnet101.h5\"  # Adjust path if needed\n",
    "model = load_model(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Provide the path to your test image\n",
    "image_path = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\\test_dir\\akiec\\ISIC_0033084.jpg\"  # Replace with your image file path\n",
    "\n",
    "# Load and preprocess the image\n",
    "input_image = load_and_preprocess_image(image_path, target_size=(224, 224))\n",
    "\n",
    "# Make a prediction\n",
    "predictions = model.predict(input_image)\n",
    "predicted_index = np.argmax(predictions, axis=1)[0]\n",
    "predicted_class = idx2class.get(predicted_index, str(predicted_index))\n",
    "print(\"Predicted class index:\", predicted_index)\n",
    "print(\"Predicted class name:\", predicted_class)\n",
    "\n",
    "# Display the image along with its predicted label\n",
    "orig_image = cv2.imread(image_path)\n",
    "orig_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(orig_image)\n",
    "plt.title(f\"Predicted: {predicted_class}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION ANY SINGLE MODEL\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# âœ… Step 1: Data Preprocessing & Custom Data Generator\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, file_paths, labels, batch_size, transform=None, shuffle=True):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.file_paths))\n",
    "        self.classes = labels\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.file_paths) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_files = [self.file_paths[i] for i in indexes]\n",
    "        batch_labels = [self.labels[i] for i in indexes]\n",
    "        images = np.array([self.__load_image(f) for f in batch_files])\n",
    "        return images, np.array(batch_labels)\n",
    "\n",
    "    def __load_image(self, image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return preprocess_input(image)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "def create_val_transform(target_size=(224, 224)):\n",
    "    height, width = target_size\n",
    "    return A.Resize(height=height, width=width)\n",
    "\n",
    "def get_data_generators(base_dir, target_size=(224, 224), batch_size=64):\n",
    "    train_dir = os.path.join(base_dir, 'train_dir')\n",
    "    val_dir = os.path.join(base_dir, 'val_dir')\n",
    "    test_dir = os.path.join(base_dir, 'test_dir')\n",
    "\n",
    "    classes = sorted(os.listdir(train_dir))\n",
    "    class_indices = {cls: i for i, cls in enumerate(classes)}\n",
    "\n",
    "    def load_images_labels(directory):\n",
    "        file_paths = []\n",
    "        labels = []\n",
    "        for class_name in classes:\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for file in sorted(os.listdir(class_path)):\n",
    "                    file_paths.append(os.path.join(class_path, file))\n",
    "                    labels.append(class_indices[class_name])\n",
    "        return np.array(file_paths), np.array(labels)\n",
    "    \n",
    "    _, _ = load_images_labels(train_dir)  # Not needed for testing\n",
    "    _, _ = load_images_labels(val_dir)  # Not needed for testing\n",
    "    test_files, test_labels = load_images_labels(test_dir)\n",
    "\n",
    "    val_transform = create_val_transform(target_size)\n",
    "\n",
    "    test_generator = CustomDataGenerator(\n",
    "        test_files, test_labels, batch_size, transform=val_transform, shuffle=False\n",
    "    )\n",
    "    test_generator.class_indices = class_indices\n",
    "\n",
    "    return test_generator, classes\n",
    "\n",
    "# âœ… Step 2: Model Evaluation Function\n",
    "def evaluate_model(model_path, test_generator, class_names):\n",
    "    print(f\"ðŸ“‚ Loading model from: {model_path}\")\n",
    "    model = load_model(model_path)\n",
    "    print(\"âœ… Model loaded successfully!\\n\")\n",
    "\n",
    "    y_true, y_pred_prob = [], []\n",
    "\n",
    "    for i in range(len(test_generator)):  # Iterate over all batches\n",
    "        images, labels = test_generator[i]  # Get batch data\n",
    "        y_true.extend(labels)\n",
    "        preds = model.predict(images, verbose=0)\n",
    "        y_pred_prob.extend(preds)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_prob = np.array(y_pred_prob)\n",
    "\n",
    "    if y_true.ndim > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    print(\"ðŸ“œ Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"\\nâœ… Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# âœ… Step 3: Load Test Generator & Evaluate\n",
    "BASE_DIR = r'D:\\\\skin_disease_detection\\\\backend\\\\own_data\\\\base_dir'\n",
    "MODEL_PATH = r\"D:\\skin_disease_detection\\trained_models\\densenet121.h5\"\n",
    "\n",
    "test_generator, class_names = get_data_generators(BASE_DIR)\n",
    "evaluate_model(MODEL_PATH, test_generator, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning trained model - Resnet101\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from utils.preprocess_3 import get_data_generators\n",
    "\n",
    "train_generator, validation_generator, test_generator, class_weights = get_data_generators()\n",
    "\n",
    "# Load the previously trained model\n",
    "fine_tune_model_path = r\"D:\\skin_disease_detection\\trained_models\\resnet101.h5\"\n",
    "model = load_model(fine_tune_model_path)\n",
    "\n",
    "# Unfreeze more layers in ResNet101 for fine-tuning (previously -40, now -100)\n",
    "for layer in model.layers[-100:]:  # Fine-tuning deeper layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Define a much lower learning rate for fine-tuning\n",
    "fine_tune_lr = 5e-6  \n",
    "lr_schedule = ExponentialDecay(fine_tune_lr, decay_steps=5000, decay_rate=0.9, staircase=True)\n",
    "\n",
    "# Compile the model with fine-tuning settings\n",
    "model.compile(optimizer=Adam(learning_rate=lr_schedule), \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Define fine-tuning callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# Fine-tune the model for a few more epochs\n",
    "fine_tune_epochs = 10  # Fine-tuning requires fewer epochs\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=fine_tune_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "fine_tuned_model_path = \"../../trained_models/resnet101_finetuned.h5\"\n",
    "model.save(fine_tuned_model_path)\n",
    "print(f\"Fine-tuned model saved at: {fine_tuned_model_path}\")\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "def evaluate_model(model, test_generator):\n",
    "    \"\"\"\n",
    "    Evaluates the model and prints classification report & confusion matrix.\n",
    "    \"\"\"\n",
    "    test_preds = model.predict(test_generator)\n",
    "    y_pred = np.argmax(test_preds, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    print(\"\\n########## CLASSIFICATION REPORT ##########\")\n",
    "    print(classification_report(y_true, y_pred, target_names=list(test_generator.class_indices.keys())))\n",
    "    \n",
    "    print(\"\\n########## CONFUSION MATRIX ##########\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Run evaluation on test data\n",
    "evaluate_model(model, test_generator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn-dis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
