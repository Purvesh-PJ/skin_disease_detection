{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET INSPECTION\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define metadata path and image directories\n",
    "metadata_path = r'D:\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_metadata.csv'\n",
    "image_dir_1 = r'D:\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_images_part_1'\n",
    "image_dir_2 = r'D:\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_images_part_2'\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Inspect the first few rows\n",
    "print(\"##### Inspect The First Few Rows #####\")\n",
    "print(metadata.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check data types\n",
    "data_types = metadata.dtypes\n",
    "print(\"##### Data Types ##### \\n\", data_types)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = metadata.isnull().sum()\n",
    "print(\"##### Missing Values In Columns ##### \\n\", missing_values)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check Unknown Valuses In \"SEX\" and \"LOCALIZATION\" Columns\n",
    "print(\"##### Checking 'unknown' values in 'SEX' and 'LOCALIZATION' columns #####\")\n",
    "if 'unknown' in metadata['sex'].values:\n",
    "    print(\"'unknown' exists in 'sex' column.\")\n",
    "else:\n",
    "    print(\"'unknown' does not exist in 'sex' column.\")\n",
    "\n",
    "if 'unknown' in metadata['localization'].values:\n",
    "    print(\"'unknown' exists in 'localization' column.\")\n",
    "else:\n",
    "    print(\"'unknown' does not exist in 'localization' column.\")\n",
    "\n",
    "# Check for duplicate rows in metadata\n",
    "print(\"\\n\")\n",
    "print(\"##### Checking Duplicate Rows #####\")\n",
    "duplicate_count = metadata.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Duplicate rows found: {duplicate_count}. Removing duplicates...\")\n",
    "    # metadata.drop_duplicates(inplace=True)\n",
    "    # print(\"Duplicate rows removed.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found. No action taken.\")\n",
    "\n",
    "# Verify that all images listed in the metadata are present in the folders\n",
    "print(\"\\n\")\n",
    "print(\"##### Checking Missing Images In Folders #####\")\n",
    "image_ids = set(metadata['image_id'])\n",
    "all_image_files = set(os.listdir(image_dir_1) + os.listdir(image_dir_2))\n",
    "missing_images = [img_id for img_id in image_ids if f\"{img_id}.jpg\" not in all_image_files]\n",
    "if missing_images:\n",
    "    print(f\"Missing images: {missing_images}\")\n",
    "else:\n",
    "    print(\"All images are accounted for.\")\n",
    "\n",
    "# Check for duplicate image_id values\n",
    "print(\"\\n\")\n",
    "# Check for rows with age = 0.0\n",
    "print(\"##### Checking for invalid ages (0.0) #####\")\n",
    "invalid_age_rows = metadata[metadata['age'] == 0.0]\n",
    "print(f\"Number of rows with invalid age: {len(invalid_age_rows)}\")\n",
    "\n",
    "if len(invalid_age_rows) > 0:\n",
    "    print(\"\\nRows with invalid age:\")\n",
    "    print(invalid_age_rows)  # Display all rows with invalid age\n",
    "else:\n",
    "    print(\"No invalid ages found.\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Basic statistics of the dataset\n",
    "print(\"\\n\")\n",
    "print(\"##### Statistics of the dataset #####\")\n",
    "print(metadata.describe(include='all'))\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET VISUALIZATION\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "metadata_path = r'D:\\projects\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_metadata.csv'\n",
    "image_dir_1 = r'D:\\projects\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_images_part_1'\n",
    "image_dir_2 = r'D:\\projects\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_images_part_2'\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Visualize the distribution of diseases\n",
    "print(\"\\n\")\n",
    "print(\"##### Displaying Distribution of Diseases (Lesion) #####\")\n",
    "sns.countplot(data=metadata, x='dx')\n",
    "plt.title(\"Distribution of Skin Diseases in HAM10000 Dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Display sample images\n",
    "print(\"##### Displaying sample images #####\")\n",
    "def display_sample_images(image_dir, num_samples=5):\n",
    "    sample_files = os.listdir(image_dir)[:num_samples]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, filename in enumerate(sample_files):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(image_dir_1)\n",
    "display_sample_images(image_dir_2)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Additional Checks and Visualizations\n",
    "\n",
    "# Correlation Analysis\n",
    "numeric_metadata = metadata.select_dtypes(include=['float64', 'int64'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(numeric_metadata.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Outliers Detection\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='age', data=metadata)\n",
    "plt.title('Age Outliers')\n",
    "plt.show()\n",
    "\n",
    "# Data Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.pairplot(metadata, hue='dx')\n",
    "plt.title('Pairplot of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET MANIPULATION\n",
    "\n",
    "import pandas as pd\n",
    "# Define metadata path and image directories\n",
    "metadata_path = r'D:\\projects\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_metadata.csv'\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# HANDLING [ UNKNOWN, MISING] VALUES IN \"SEX\" AND \"LOCALIZATION\" COLUMNS\n",
    "if 'unknown' in metadata['sex'].values:\n",
    "    metadata['sex'].replace('unknown', metadata['sex'].mode()[0], inplace=True)\n",
    "    print(f\"'unknown' in 'sex' replaced with mode: {metadata['sex'].mode()[0]}\")\n",
    "\n",
    "if 'unknown' in metadata['localization'].values:\n",
    "    metadata['localization'].replace('unknown', metadata['localization'].mode()[0], inplace=True)\n",
    "    print(f\"'unknown' in 'localization' replaced with mode: {metadata['localization'].mode()[0]}\")\n",
    "\n",
    "# Check for rows with age = 0.0\n",
    "print(\"##### Checking for invalid ages (0.0) #####\")\n",
    "invalid_age_rows = metadata[metadata['age'] == 0.0]\n",
    "print(f\"Number of rows with invalid age: {len(invalid_age_rows)}\")\n",
    "\n",
    "if len(invalid_age_rows) > 0:\n",
    "    print(\"\\nRows with invalid age:\")\n",
    "    print(invalid_age_rows)  # Display all rows with invalid age\n",
    "\n",
    "    # Replace 0.0 with the median age\n",
    "    median_age = metadata[metadata['age'] > 0.0]['age'].median()\n",
    "    metadata['age'] = metadata['age'].replace(0.0, median_age)\n",
    "    print(f\"Invalid ages replaced with the median age: {median_age:.2f}\")\n",
    "else:\n",
    "    print(\"No invalid ages found.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Save the cleaned metadata\n",
    "metadata.to_csv(metadata_path, index=False)\n",
    "print(\"Cleaned metadata saved to metadata.csv : {metadata_path}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING CLASS WEIGHTS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "metadata_path = r'D:\\projects\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_metadata.csv'\n",
    "dataset = pd.read_csv(metadata_path)\n",
    "labels = dataset['dx']\n",
    "\n",
    "print(labels.head())\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Define class labels and counts from your dataset\n",
    "classes = ['bkl', 'nv', 'df', 'mel', 'vasc', 'bcc', 'akiec']\n",
    "class_counts = [1099, 6705, 115, 1113, 142, 514, 327]  # Replace with your dataset values\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA VISUALIZATION : DATA2 CLASS DISTRIBUTION PLOT\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "def plot_class_distribution(dataset_path):\n",
    "    \"\"\"\n",
    "    Visualizes the class distribution from the dataset folder structure.\n",
    "    \n",
    "    :param dataset_path: Path to the dataset directory containing class subfolders.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    \n",
    "    # Iterate through each class folder\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_folder = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_folder):\n",
    "            class_counts[class_name] = len(os.listdir(class_folder))\n",
    "    \n",
    "    # Plot the distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()), palette=\"viridis\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of Images\")\n",
    "    plt.title(\"Dataset Class Distribution\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (update with your actual dataset path)\n",
    "dataset_path = r\"D:\\skin_disease_detection\\backend\\data2\\base_dir\\train_dir\"\n",
    "plot_class_distribution(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SCRIPT INCREASE ALL CLASSES 200 PLUS FOR BALANCE VAL_DIR IN DATA2\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def augment_images_balanced(val_dir, target_count=500, max_variants_per_image=5):\n",
    "    \"\"\"\n",
    "    Augments images inside val_dir while ensuring balanced class distribution.\n",
    "    - Each class will have at least `target_count` images.\n",
    "    - Prevents excessive duplication from a single image.\n",
    "    - Saves augmented images with `_X_randomnumber.jpg` format (X = class 0-6).\n",
    "    \"\"\"\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='reflect'\n",
    "    )\n",
    "\n",
    "    for class_index, class_name in enumerate(os.listdir(val_dir)):\n",
    "        class_path = os.path.join(val_dir, class_name)\n",
    "\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # Skip non-folder files\n",
    "\n",
    "        images = [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
    "        num_existing = len(images)\n",
    "\n",
    "        if num_existing >= target_count:\n",
    "            print(f\"Skipping {class_name}, already has {num_existing} images.\")\n",
    "            continue\n",
    "\n",
    "        num_needed = target_count - num_existing\n",
    "        print(f\"Generating {num_needed} images for class: {class_name}\")\n",
    "\n",
    "        # Determine how many times each image should be augmented\n",
    "        augmentation_distribution = np.full(len(images), num_needed // len(images))\n",
    "        augmentation_distribution[:num_needed % len(images)] += 1  # Spread remainder\n",
    "\n",
    "        for img_path, aug_count in zip(images, augmentation_distribution):\n",
    "            if aug_count == 0:\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "            i = 0\n",
    "            for batch in datagen.flow(img, batch_size=1):\n",
    "                random_number = np.random.randint(10000, 99999)\n",
    "                filename = f\"_{class_index}_{random_number}.jpg\"\n",
    "                save_path = os.path.join(class_path, filename)\n",
    "\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(batch[0].astype('uint8'), cv2.COLOR_RGB2BGR))\n",
    "                i += 1\n",
    "                if i >= aug_count or i >= max_variants_per_image:\n",
    "                    break  # Prevent excessive augmentation per image\n",
    "\n",
    "    print(\"Augmentation completed.\")\n",
    "\n",
    "# Example usage:\n",
    "val_directory = r\"D:\\skin_disease_detection\\backend\\data2\\base_dir\\val_dir\"\n",
    "augment_images_balanced(val_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SCRIPT ONLY INCREASE CLASSES WHOSE LESS THAN 200 AND BALANCE VAL_DIR IN DATA2\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def augment_images_balanced(val_dir, min_target=450, max_variants_per_image=10):\n",
    "    \"\"\"\n",
    "    Augments images in `val_dir` to ensure every class has at least `min_target` images.\n",
    "    - Classes with fewer images will get more augmentation.\n",
    "    - Limits excessive duplication per image using `max_variants_per_image`.\n",
    "    \"\"\"\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='reflect'\n",
    "    )\n",
    "\n",
    "    for class_index, class_name in enumerate(os.listdir(val_dir)):\n",
    "        class_path = os.path.join(val_dir, class_name)\n",
    "\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # Skip non-folder files\n",
    "\n",
    "        images = [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
    "        num_existing = len(images)\n",
    "\n",
    "        if num_existing >= min_target:\n",
    "            print(f\"âœ… Skipping {class_name}, already has {num_existing} images.\")\n",
    "            continue\n",
    "\n",
    "        num_needed = min_target - num_existing\n",
    "        print(f\"ðŸ”„ Generating {num_needed} images for class: {class_name}\")\n",
    "\n",
    "        # Adjust max augmentations per image based on how few images we have\n",
    "        per_image_limit = min(max_variants_per_image, (num_needed // max(1, num_existing)) + 1)\n",
    "\n",
    "        for img_path in images:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "            img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "            i = 0\n",
    "            for batch in datagen.flow(img, batch_size=1):\n",
    "                random_number = np.random.randint(10000, 99999)\n",
    "                filename = f\"_{class_index}_{random_number}.jpg\"\n",
    "                save_path = os.path.join(class_path, filename)\n",
    "\n",
    "                cv2.imwrite(save_path, cv2.cvtColor(batch[0].astype('uint8'), cv2.COLOR_RGB2BGR))\n",
    "                i += 1\n",
    "                if i >= per_image_limit:\n",
    "                    break  # Stop once enough images are generated\n",
    "\n",
    "    print(\"âœ… Augmentation completed with balanced validation set.\")\n",
    "\n",
    "# Example usage:\n",
    "val_directory = r\"D:\\skin_disease_detection\\backend\\data2\\base_dir\\val_dir\"\n",
    "augment_images_balanced(val_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRIPT FOR SPLITTING DATASET INTO TRAIN, VALIDATION, AND TEST SETS USING ORIGINAL HAM10000 DATASET\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "BASE_DIR = r\"D:\\skin_disease_detection\\backend\\data\\Ham10000\"\n",
    "IMAGES_DIR_1 = os.path.join(BASE_DIR, \"HAM10000_images_part_1\")\n",
    "IMAGES_DIR_2 = os.path.join(BASE_DIR, \"HAM10000_images_part_2\")\n",
    "METADATA_FILE = os.path.join(BASE_DIR, \"HAM10000_metadata.csv\")\n",
    "\n",
    "# Define new dataset structure\n",
    "OUTPUT_DIR = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\"\n",
    "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train_dir\")\n",
    "VAL_DIR = os.path.join(OUTPUT_DIR, \"val_dir\")\n",
    "TEST_DIR = os.path.join(OUTPUT_DIR, \"test_dir\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "for split_dir in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    for class_name in [\"akiec\", \"bcc\", \"bkl\", \"df\", \"mel\", \"nv\", \"vasc\"]:\n",
    "        os.makedirs(os.path.join(split_dir, class_name), exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(METADATA_FILE)\n",
    "\n",
    "# Combine image paths\n",
    "metadata[\"image_path\"] = metadata[\"image_id\"].apply(\n",
    "    lambda x: os.path.join(IMAGES_DIR_1, x + \".jpg\") if os.path.exists(os.path.join(IMAGES_DIR_1, x + \".jpg\"))\n",
    "    else os.path.join(IMAGES_DIR_2, x + \".jpg\")\n",
    ")\n",
    "\n",
    "# Split dataset (70% train, 20% val, 10% test) ensuring no overlap\n",
    "train_data, temp_data = train_test_split(metadata, test_size=0.3, stratify=metadata[\"dx\"], random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=1/3, stratify=temp_data[\"dx\"], random_state=42)\n",
    "\n",
    "def copy_images(df, dest_dir):\n",
    "    \"\"\" Copy images to the respective directory \"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        label = row[\"dx\"]  # Lesion label\n",
    "        src_path = row[\"image_path\"]\n",
    "        dest_path = os.path.join(dest_dir, label, os.path.basename(src_path))\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Copy images to respective directories\n",
    "copy_images(train_data, TRAIN_DIR)\n",
    "copy_images(val_data, VAL_DIR)\n",
    "copy_images(test_data, TEST_DIR)\n",
    "\n",
    "print(\"Dataset successfully split into train, validation, and test sets without overlap!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING INTERSECTION BETWEEN TRAIN, VAL, AND TEST DIRECTORIES\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset paths\n",
    "METADATA_FILE = r\"D:\\skin_disease_detection\\backend\\data\\Ham10000\\HAM10000_metadata.csv\"\n",
    "TRAIN_DIR = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\\train_dir\"\n",
    "VAL_DIR = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\\val_dir\"\n",
    "TEST_DIR = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\\test_dir\"\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(METADATA_FILE)\n",
    "\n",
    "# Function to get image IDs from directory\n",
    "def get_image_ids(directory):\n",
    "    image_ids = set()\n",
    "    for class_name in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            for image in os.listdir(class_path):\n",
    "                image_id = os.path.splitext(image)[0]  # Remove file extension\n",
    "                image_ids.add(image_id)\n",
    "    return image_ids\n",
    "\n",
    "# Get image IDs from each dataset\n",
    "train_ids = get_image_ids(TRAIN_DIR)\n",
    "val_ids = get_image_ids(VAL_DIR)\n",
    "test_ids = get_image_ids(TEST_DIR)\n",
    "\n",
    "# Check for intersections\n",
    "train_val_intersection = train_ids.intersection(val_ids)\n",
    "train_test_intersection = train_ids.intersection(test_ids)\n",
    "val_test_intersection = val_ids.intersection(test_ids)\n",
    "\n",
    "# Print results\n",
    "if not train_val_intersection and not train_test_intersection and not val_test_intersection:\n",
    "    print(\"No intersections found between train, val, and test sets. Data split is clean.\")\n",
    "else:\n",
    "    if train_val_intersection:\n",
    "        print(f\"Train-Val Intersection Detected: {len(train_val_intersection)} images\")\n",
    "    if train_test_intersection:\n",
    "        print(f\"Train-Test Intersection Detected: {len(train_test_intersection)} images\")\n",
    "    if val_test_intersection:\n",
    "        print(f\"Val-Test Intersection Detected: {len(val_test_intersection)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SCRIPT FOR BALANCING TRAIN DATASET USING AUGMENTATION\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "\n",
    "# Define the train directory\n",
    "TRAIN_DIR = r\"D:\\skin_disease_detection\\backend\\own_data\\base_dir\\train_dir\"\n",
    "\n",
    "# Define class sample counts\n",
    "class_counts = {\n",
    "    \"akiec\": 229,\n",
    "    \"bcc\": 360,\n",
    "    \"bkl\": 769,\n",
    "    \"df\": 81,\n",
    "    \"mel\": 779,\n",
    "    \"nv\": 4693,  # Maximum samples\n",
    "    \"vasc\": 99\n",
    "}\n",
    "\n",
    "MAX_SAMPLES = max(class_counts.values())\n",
    "\n",
    "# Define Albumentations augmentation pipeline\n",
    "AUGMENTATION = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, border_mode=cv2.BORDER_REFLECT, p=0.7),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "    A.Resize(height=224, width=224, p=1.0)  # Resizing to 512x512 for better feature retention\n",
    "])\n",
    "\n",
    "def augment_image(image_path, output_path):\n",
    "    \"\"\"Applies advanced augmentation using Albumentations and saves the image.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    augmented = AUGMENTATION(image=image)['image']\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "def balance_classes():\n",
    "    \"\"\"Balances the dataset by oversampling minority classes to reach MAX_SAMPLES.\"\"\"\n",
    "    for class_name, count in class_counts.items():\n",
    "        class_dir = os.path.join(TRAIN_DIR, class_name)\n",
    "        images = [img for img in os.listdir(class_dir) if img.endswith(\".jpg\")]\n",
    "        num_images = len(images)\n",
    "        \n",
    "        if num_images < MAX_SAMPLES:\n",
    "            needed = MAX_SAMPLES - num_images\n",
    "            print(f\"Oversampling {class_name}: Adding {needed} images.\")\n",
    "            \n",
    "            for i in range(needed):\n",
    "                img_choice = random.choice(images)\n",
    "                src_path = os.path.join(class_dir, img_choice)\n",
    "                new_img_name = f\"aug_{i}_{img_choice}\"\n",
    "                dest_path = os.path.join(class_dir, new_img_name)\n",
    "                \n",
    "                augment_image(src_path, dest_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    balance_classes()\n",
    "    print(\"Train dataset balanced successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skn-dis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
